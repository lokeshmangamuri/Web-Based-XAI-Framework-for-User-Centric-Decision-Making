{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "cf50IXcKmwqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0qkKxvCOtXoA",
        "outputId": "41ed3800-0822-45ef-9236-3f653a29521a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0+cu126'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modeltype = 'mobilenet'\n",
        "\n",
        "ds = 'sick_ones_bendbias_v3_2class_normal'\n",
        "eval_ds = 'sick_ones_bendbias_v3_2class_variation'\n",
        "\n",
        "random_state = 42"
      ],
      "metadata": {
        "id": "t4avVLLDm16s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Yv2B_aFLxqip",
        "outputId": "428e94a8-2231-447c-8335-35af3629f1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relative_model_path = \"two4two_sickones_models_pytorch\"\n",
        "base_path = Path('/content/drive/MyDrive') / relative_model_path\n",
        "base_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLMDbRRVnS5l",
        "outputId": "d104d7b7-3c20-41af-f913-3cbbfd5ed05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/two4two_sickones_models_pytorch')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ0c-TWNxIzO"
      },
      "outputs": [],
      "source": [
        "# data downloading and dataset utilities\n",
        "\n",
        "def download_file(url, file_name, cache_dir=\"data\", extract=True, force_download=False, archive_folder=None):\n",
        "    # Ensure the cache directory exists\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    file_path = os.path.join(cache_dir, file_name)\n",
        "\n",
        "    # Download the file\n",
        "    if not os.path.exists(file_path) or force_download:\n",
        "      torch.hub.download_url_to_file(url, file_path)\n",
        "      print(f\"File downloaded to: {file_path}\")\n",
        "    else:\n",
        "      print(f\"File already exists at: {file_path}\")\n",
        "\n",
        "    if extract:\n",
        "      with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "          tar.extractall(path=cache_dir)\n",
        "      print(f\"File extracted to: {cache_dir}\")\n",
        "      return Path(cache_dir) / archive_folder if archive_folder is not None else Path(cache_dir)\n",
        "    elif archive_folder is not None and (Path(cache_dir) / archive_folder).exsists:\n",
        "      return Path(cache_dir) / archive_folder\n",
        "    else:\n",
        "      return Path(cache_dir)\n",
        "\n",
        "    return Path(file_path)\n",
        "\n",
        "def load_dataframe(data_dir, dataset):\n",
        "  data_dir = data_dir / dataset\n",
        "  df = pd.read_json(data_dir / 'parameters.jsonl', lines=True)\n",
        "  df['filename'] = df['id'] + '.png'\n",
        "  #df['ill'] = df['ill'].astype(int).astype(str)\n",
        "\n",
        "  return df\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None, target_columns=None):\n",
        "        self.df = df\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_columns = target_columns if target_columns is not None else ['spherical', 'ill_spherical', 'bending', 'arm_position']\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.df.iloc[idx]['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        # Load the specified target columns instead of just 'ill'\n",
        "        targets = self.df.iloc[idx][self.target_columns].values.astype(np.float32)\n",
        "        targets = torch.tensor(targets)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download datafrom sciebo\n",
        "data_dir = download_file(url=\"https://osf.io/download/kexzt/?view_only=adcc520b88cc4ea3b8236c5178ba3ab5\",\n",
        "                         file_name=\"blockies_datasets.tar.gz\",\n",
        "                         cache_dir='/content/data', # change this if not using Colab\n",
        "                         extract=True,\n",
        "                         force_download=False,\n",
        "                         archive_folder='blockies_datasets')\n",
        "data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zwFR5u7n1OI",
        "outputId": "884b3842-2e33-4863-c721-b3797b50ab59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.00G/1.00G [00:26<00:00, 41.3MB/s]\n",
            "/tmp/ipython-input-3849801842.py:17: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=cache_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded to: /content/data/blockies_datasets.tar.gz\n",
            "File extracted to: /content/data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/data/blockies_datasets')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df_reduced = load_dataframe(ds_dir, 'train')\n",
        "train_transforms = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "train_df_reduced_filtered = filter_existing_images(train_df_reduced, ds_dir / 'train')\n",
        "val_df_reduced_filtered = filter_existing_images(val_df_reduced, ds_dir / 'validation')\n",
        "test_df_reduced_filtered = filter_existing_images(test_df_reduced, ds_dir / 'test')\n",
        "eval_df_reduced_filtered = filter_existing_images(eval_df_reduced, eval_ds_dir / 'test')\n",
        "\n",
        "train_dataset_for_norm = ImageDataset(train_df_reduced, ds_dir / 'train', transform=train_transforms)\n",
        "dataloader_for_norm = DataLoader(train_dataset_for_norm, batch_size=100, shuffle=True,\n",
        "                        num_workers=6, pin_memory=True)\n",
        "\n",
        "# Initialize variables to calculate mean\n",
        "mean = torch.zeros(3)  # For RGB channels\n",
        "total_pixels = 0\n",
        "\n",
        "# Loop through the dataset\n",
        "for images, _ in tqdm(dataloader_for_norm):\n",
        "    # Sum pixel values per channel\n",
        "    mean += images.sum(dim=[0, 2, 3])\n",
        "    total_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "# Divide by total number of pixels\n",
        "mean /= total_pixels\n",
        "\n",
        "print(f\"Mean per channel: {mean}\")\n",
        "\n",
        "# Initialize variables for std calculation\n",
        "std = torch.zeros(3)\n",
        "\n",
        "# Loop again for standard deviation\n",
        "for images, _ in tqdm(dataloader_for_norm):\n",
        "    std += ((images - mean.view(1, 3, 1, 1))**2).sum(dim=[0, 2, 3])\n",
        "\n",
        "std = torch.sqrt(std / total_pixels)\n",
        "\n",
        "print(f\"Standard Deviation per channel: {std}\")"
      ],
      "metadata": {
        "id": "I5JYWuuE1bR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd91164d-61b4-425a-da4d-4fcf95f9607a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [00:13<00:00, 28.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean per channel: tensor([0.8068, 0.7830, 0.8005])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [00:13<00:00, 28.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Deviation per channel: tensor([0.1093, 0.1136, 0.1029])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive Data Augmentation Pipeline\n",
        "transform_train = T.Compose([\n",
        "    # Resize and Crop\n",
        "    T.RandomResizedCrop(128, scale=(0.8, 1.0)),\n",
        "    # Flipping\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomVerticalFlip(p=0.1),\n",
        "    # Rotation\n",
        "    T.RandomRotation(degrees=30),\n",
        "    # Affine Transformations\n",
        "    T.RandomAffine(\n",
        "        degrees=15,\n",
        "        translate=(0.1, 0.1),\n",
        "        scale=(0.8, 1.2),\n",
        "        shear=10\n",
        "    ),\n",
        "    # Perspective Transformation\n",
        "    T.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
        "    # Color Augmentations\n",
        "    T.ColorJitter(\n",
        "        brightness=0.4,\n",
        "        contrast=0.4,\n",
        "        saturation=0.4,\n",
        "        hue=0.1\n",
        "    ),\n",
        "    # Grayscale\n",
        "    T.RandomGrayscale(p=0.1),\n",
        "    # Blur\n",
        "    T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "    T.ToTensor(),\n",
        "    # Random Erasing\n",
        "    T.RandomErasing(\n",
        "        p=0.5,\n",
        "        scale=(0.02, 0.33),\n",
        "        ratio=(0.3, 3.3)\n",
        "    ),\n",
        "    T.Normalize(mean=mean, std=std)\n",
        "])\n",
        "# load datasets and dataloaders for Training and Evaluation\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std)\n",
        "])"
      ],
      "metadata": {
        "id": "F8pEAh6FmtfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_df_reduced), len(test_df_reduced), len(eval_df_reduced), len(train_df_reduced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SupilWZ5n5Z",
        "outputId": "5ed5d160-3f42-4cb1-9d1e-01be488e17ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 3000, 3000, 40000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correcting the data_dir for validation, test, and eval datasets\n",
        "train_dataset = ImageDataset(train_df_reduced, ds_dir / 'train', transform=transform_train, target_columns=regression_target_columns)\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                              num_workers=6, pin_memory=True)\n",
        "\n",
        "train_eval_dataset = ImageDataset(train_df_reduced, ds_dir / 'train', transform=transform, target_columns=regression_target_columns)\n",
        "print(f\"Number of training eval samples: {len(train_eval_dataset)}\")\n",
        "train_eval_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False,\n",
        "                                   num_workers=6, pin_memory=True)\n",
        "\n",
        "val_dataset = ImageDataset(val_df_reduced,  ds_dir / 'validation', transform=transform, target_columns=regression_target_columns)\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
        "                            num_workers=6, pin_memory=True)\n",
        "\n",
        "test_dataset = ImageDataset(test_df_reduced,  ds_dir / 'test' , transform=transform, target_columns=regression_target_columns)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)\n",
        "\n",
        "eval_dataset = ImageDataset(eval_df_reduced,  eval_ds_dir / 'test', transform=transform, target_columns=regression_target_columns)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False,\n",
        "                             num_workers=6, pin_memory=True)"
      ],
      "metadata": {
        "id": "XAj322ANzPEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d2e299-1e96-40df-a968-f13be4c29b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 40000\n",
            "Number of training eval samples: 40000\n",
            "Number of validation samples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_ex = next(iter(train_dataloader))\n",
        "data_ex[0].shape, data_ex[1].shape"
      ],
      "metadata": {
        "id": "X16kEjZyzYYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824129fe-a396-44fc-9a92-b8648e6cc18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3, 128, 128]), torch.Size([32, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model"
      ],
      "metadata": {
        "id": "1nWFInyQdS5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mobilenetv2_regression(num_targets, pretrained=True, checkpoint_path=None):\n",
        "  \"\"\"Loads a MobileNetV2 model for regression, optionally loading from a checkpoint.\n",
        "\n",
        "  Args:\n",
        "    num_targets: The number of output regression targets.\n",
        "    pretrained: Whether to load the pre-trained weights.\n",
        "    checkpoint_path: Path to a checkpoint file to load.\n",
        "\n",
        "  Returns:\n",
        "    A MobileNetV2 model configured for regression.\n",
        "  \"\"\"\n",
        "  model = models.mobilenet_v2(weights=None if not pretrained else 'DEFAULT')\n",
        "  model.classifier[1] = nn.Linear(model.last_channel, num_targets)\n",
        "\n",
        "  if checkpoint_path:\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu')) # Load to CPU first\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        print(\"Checkpoint might not match the model architecture. Starting training without loading checkpoint.\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "dOmvP6Qc2HMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  \"\"\"\n",
        "  Sets random seeds for reproducibility.\n",
        "  \"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Ensure r2_score is imported\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device, num_targets):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate R-squared for each target\n",
        "    r2_scores = []\n",
        "    for i in range(num_targets):\n",
        "        r2 = r2_score(all_labels[:, i], all_preds[:, i])\n",
        "        r2_scores.append(r2)\n",
        "\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
        "    for i, r2 in enumerate(r2_scores):\n",
        "        print(f\"R-squared for target {i}: {r2:.4f}\")\n",
        "    return all_preds, avg_loss, r2_scores\n",
        "\n",
        "def train_model(model, dl_train, dl_val, criterion, optimizer, scheduler, device, checkpoint_path, num_epochs=5, num_targets=4):\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  best_val_loss = sys.float_info.max\n",
        "  best_epoch = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    # Wrap the training dataloader with tqdm for progress visualization\n",
        "    for inputs, labels in tqdm(dl_train, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        # print(inputs.shape, labels.shape, inputs.min(), inputs.max(), inputs.mean())\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track training loss and accuracy\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    train_loss = running_train_loss / len(dl_train)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dl_val, desc=f\"Validation Epoch {epoch+1}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track validation loss and accuracy\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "    val_loss = running_val_loss / len(dl_val)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
        "    print(f\"\\tValidation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Checkpointing the best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        print(f\"New best model found at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(model.state_dict(), checkpoint_path / 'tmp' / 'best_model.pth')  # Save only the model's state_dict\n",
        "\n",
        "  # To load the best model later:\n",
        "  model = load_mobilenetv2_regression(num_targets=num_targets,\n",
        "                                        pretrained=False,\n",
        "                                        checkpoint_path=checkpoint_path / 'tmp' / 'best_model.pth')\n",
        "  model.to(device)\n",
        "\n",
        "  # Evaluate the best model on the validation set\n",
        "  _, val_loss, val_r2_scores = evaluate_model(model, dl_val, criterion, device, num_targets)\n",
        "\n",
        "\n",
        "  print(f\"Training Run complete! Val loss = {best_val_loss:.4f} | Val R-squared (avg) = {np.mean(val_r2_scores):.4f} | Epoch = {best_epoch}\", )\n",
        "  print(\"-\" * 30)\n",
        "\n",
        "  return model, val_loss, np.mean(val_r2_scores)"
      ],
      "metadata": {
        "id": "-CrtqyOl2jt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup model path\n",
        "model_path = base_path / ds / f'{modeltype}_regression'\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Model path:\", model_path)\n",
        "\n",
        "# setup checkpoint folders\n",
        "checkpoint_path = model_path / \"torch_mobilenetv2/\"\n",
        "(checkpoint_path / 'tmp').mkdir(parents=True, exist_ok=True)\n",
        "(checkpoint_path / 'final').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define loss function (using MSELoss for regression)\n",
        "criterion = nn.MSELoss(reduction='sum') # Or nn.L1Loss()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "# Define the number of regression targets\n",
        "num_regression_targets = len(regression_target_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoD0pL8rE_Bd",
        "outputId": "f026c09b-4031-48c7-ac9c-41533b5b5f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /content/drive/MyDrive/two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet_regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of train_df: {train_df_reduced.shape}\")\n",
        "print(f\"Shape of val_df: {val_df_reduced.shape}\")\n",
        "print(f\"Shape of test_df: {test_df_reduced.shape}\")\n",
        "print(f\"Shape of eval_df: {eval_df_reduced.shape}\")"
      ],
      "metadata": {
        "id": "a-IV2oGm2zot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8c4f44-7e4a-4057-8e92-993f8d8b96ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_df: (40000, 6)\n",
            "Shape of val_df: (1000, 6)\n",
            "Shape of test_df: (3000, 6)\n",
            "Shape of eval_df: (3000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def check_missing_files(df, directory):\n",
        "    missing = []\n",
        "    for fname in df['filename']:\n",
        "        if not (Path(directory) / fname).exists():\n",
        "            missing.append(fname)\n",
        "    print(f\"Missing files: {len(missing)}\")\n",
        "    if missing:\n",
        "        print(\"Examples of missing files:\", missing[:5])\n",
        "    return missing\n",
        "\n",
        "# Run this for all datasets\n",
        "missing_val = check_missing_files(val_df_reduced, ds_dir / 'validation')\n",
        "missing_train = check_missing_files(train_df_reduced, ds_dir / 'train')\n",
        "missing_test = check_missing_files(test_df_reduced, ds_dir / 'test')\n",
        "missing_eval = check_missing_files(eval_df_reduced, eval_ds_dir / 'test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_X_gXl1JjKn",
        "outputId": "a94693ca-cb58-4ed9-e87a-8a3bd928721e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing files: 0\n",
            "Missing files: 0\n",
            "Missing files: 0\n",
            "Missing files: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run training\n",
        "n_runs = 1\n",
        "n_epochs = 50\n",
        "\n",
        "load_checkpoints = False\n",
        "learning_rate = 0.001\n",
        "\n",
        "best_val_loss = sys.float_info.max\n",
        "for i in range(n_runs):\n",
        "\n",
        "  set_seed(42 + i)\n",
        "\n",
        "  print(f\"Run {i+1} / {n_runs}\")\n",
        "  print(\"=\" * 30)\n",
        "\n",
        "  #if i > 0:\n",
        "   # print('loading previous checkpoint with augmentation')\n",
        "    #load_checkpoints = True\n",
        "\n",
        "  #if i >= 0:\n",
        "    # load previous checkpoint and train without augmentation\n",
        "   # print('Loading previous checkpoint and training with out augmentation')\n",
        "    #train_dataset = ImageDataset(train_df, ds_dir / 'train', transform=transform)\n",
        "    #train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                                 # num_workers=6, pin_memory=True)\n",
        "\n",
        "\n",
        "  # Load the regression model\n",
        "  model = load_mobilenetv2_regression(num_regression_targets, pretrained=False, checkpoint_path= checkpoint_path / 'final' / 'best_model.pth' if load_checkpoints else None)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=True)\n",
        "\n",
        "  # Scheduler\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=4,\n",
        "                                                          threshold=0.01, threshold_mode='abs')\n",
        "\n",
        " # Train the regression model\n",
        "  model, val_loss, val_avg_r2 = train_model(model,\n",
        "                                         train_dataloader, val_dataloader,\n",
        "                                         criterion, optimizer, scheduler,\n",
        "                                         device, checkpoint_path,\n",
        "                                         num_epochs=n_epochs,\n",
        "                                         num_targets=num_regression_targets)\n",
        "\n",
        "  # Checkpointing the best model\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      print(f\"New best model found at Run {i+1} with validation loss: {val_loss:.4f}\")\n",
        "      torch.save(model.state_dict(), checkpoint_path / 'final' / 'best_model.pth')  # Save only the model's state_dict\n",
        "  print()\n",
        "\n",
        "# Load best model:\n",
        "model = load_mobilenetv2_regression(num_targets=num_regression_targets,\n",
        "                         pretrained=False,\n",
        "                         checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the final best model on the validation set\n",
        "_, val_loss, val_r2_scores = evaluate_model(model, val_dataloader, criterion, device, num_regression_targets)\n",
        "\n",
        "print(f\"Training complete! Final Val loss = {val_loss:.4f} | Final Val R-squared (avg) = {np.mean(val_r2_scores):.4f}\", )\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "SWtoWl6K2k96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31750f76-52e9-4ac4-9e78-d441b465c7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 / 1\n",
            "==============================\n",
            "Epoch [1/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 1250/1250 [00:58<00:00, 21.43it/s]\n",
            "Validation Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 45.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 11.7497\n",
            "\tValidation Loss: 10.3098\n",
            "New best model found at epoch 1 with validation loss: 10.3098\n",
            "Epoch [2/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 1250/1250 [00:56<00:00, 22.04it/s]\n",
            "Validation Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 45.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 9.2526\n",
            "\tValidation Loss: 7.5730\n",
            "New best model found at epoch 2 with validation loss: 7.5730\n",
            "Epoch [3/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 1250/1250 [00:56<00:00, 22.04it/s]\n",
            "Validation Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 46.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 7.7054\n",
            "\tValidation Loss: 5.6834\n",
            "New best model found at epoch 3 with validation loss: 5.6834\n",
            "Epoch [4/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 1250/1250 [00:57<00:00, 21.85it/s]\n",
            "Validation Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 43.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 7.1033\n",
            "\tValidation Loss: 4.9064\n",
            "New best model found at epoch 4 with validation loss: 4.9064\n",
            "Epoch [5/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 1250/1250 [00:57<00:00, 21.84it/s]\n",
            "Validation Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 45.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 6.7850\n",
            "\tValidation Loss: 4.6685\n",
            "New best model found at epoch 5 with validation loss: 4.6685\n",
            "Epoch [6/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 1250/1250 [00:57<00:00, 21.62it/s]\n",
            "Validation Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 41.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 6.5504\n",
            "\tValidation Loss: 4.6524\n",
            "New best model found at epoch 6 with validation loss: 4.6524\n",
            "Epoch [7/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 1250/1250 [00:56<00:00, 22.00it/s]\n",
            "Validation Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 48.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 6.3162\n",
            "\tValidation Loss: 4.4775\n",
            "New best model found at epoch 7 with validation loss: 4.4775\n",
            "Epoch [8/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 1250/1250 [00:56<00:00, 21.93it/s]\n",
            "Validation Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 45.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 6.0894\n",
            "\tValidation Loss: 4.7105\n",
            "Epoch [9/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 1250/1250 [00:57<00:00, 21.78it/s]\n",
            "Validation Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 49.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 5.8672\n",
            "\tValidation Loss: 3.9041\n",
            "New best model found at epoch 9 with validation loss: 3.9041\n",
            "Epoch [10/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 1250/1250 [00:57<00:00, 21.79it/s]\n",
            "Validation Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 49.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 5.6705\n",
            "\tValidation Loss: 4.2500\n",
            "Epoch [11/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 1250/1250 [00:56<00:00, 22.01it/s]\n",
            "Validation Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 49.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 5.4059\n",
            "\tValidation Loss: 3.3557\n",
            "New best model found at epoch 11 with validation loss: 3.3557\n",
            "Epoch [12/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 1250/1250 [00:56<00:00, 21.96it/s]\n",
            "Validation Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 48.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 5.1888\n",
            "\tValidation Loss: 3.1169\n",
            "New best model found at epoch 12 with validation loss: 3.1169\n",
            "Epoch [13/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 1250/1250 [00:57<00:00, 21.89it/s]\n",
            "Validation Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 46.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 4.9937\n",
            "\tValidation Loss: 3.1804\n",
            "Epoch [14/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 1250/1250 [00:56<00:00, 21.96it/s]\n",
            "Validation Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 47.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 4.7673\n",
            "\tValidation Loss: 2.5797\n",
            "New best model found at epoch 14 with validation loss: 2.5797\n",
            "Epoch [15/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 1250/1250 [00:57<00:00, 21.84it/s]\n",
            "Validation Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 44.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 4.5667\n",
            "\tValidation Loss: 2.8250\n",
            "Epoch [16/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 1250/1250 [00:57<00:00, 21.80it/s]\n",
            "Validation Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 47.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 4.4161\n",
            "\tValidation Loss: 2.0269\n",
            "New best model found at epoch 16 with validation loss: 2.0269\n",
            "Epoch [17/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 1250/1250 [00:56<00:00, 22.03it/s]\n",
            "Validation Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 44.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 4.2456\n",
            "\tValidation Loss: 2.1245\n",
            "Epoch [18/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 1250/1250 [00:57<00:00, 21.74it/s]\n",
            "Validation Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 48.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 4.1350\n",
            "\tValidation Loss: 3.2325\n",
            "Epoch [19/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19: 100%|██████████| 1250/1250 [00:56<00:00, 21.93it/s]\n",
            "Validation Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 45.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.9925\n",
            "\tValidation Loss: 2.0602\n",
            "Epoch [20/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 20: 100%|██████████| 1250/1250 [00:56<00:00, 22.14it/s]\n",
            "Validation Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 46.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.8593\n",
            "\tValidation Loss: 1.7861\n",
            "New best model found at epoch 20 with validation loss: 1.7861\n",
            "Epoch [21/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 21: 100%|██████████| 1250/1250 [00:56<00:00, 21.94it/s]\n",
            "Validation Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 46.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.8091\n",
            "\tValidation Loss: 2.3042\n",
            "Epoch [22/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 22: 100%|██████████| 1250/1250 [00:56<00:00, 22.11it/s]\n",
            "Validation Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 45.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.6892\n",
            "\tValidation Loss: 2.0953\n",
            "Epoch [23/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 23: 100%|██████████| 1250/1250 [00:57<00:00, 21.82it/s]\n",
            "Validation Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 44.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.5794\n",
            "\tValidation Loss: 2.7149\n",
            "Epoch [24/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 24: 100%|██████████| 1250/1250 [00:57<00:00, 21.79it/s]\n",
            "Validation Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 50.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.5500\n",
            "\tValidation Loss: 1.8643\n",
            "Epoch [25/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 25: 100%|██████████| 1250/1250 [00:57<00:00, 21.85it/s]\n",
            "Validation Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 47.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.4996\n",
            "\tValidation Loss: 1.5198\n",
            "New best model found at epoch 25 with validation loss: 1.5198\n",
            "Epoch [26/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 26: 100%|██████████| 1250/1250 [00:56<00:00, 22.19it/s]\n",
            "Validation Epoch 26: 100%|██████████| 32/32 [00:00<00:00, 49.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.4187\n",
            "\tValidation Loss: 1.2515\n",
            "New best model found at epoch 26 with validation loss: 1.2515\n",
            "Epoch [27/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 27: 100%|██████████| 1250/1250 [00:57<00:00, 21.76it/s]\n",
            "Validation Epoch 27: 100%|██████████| 32/32 [00:00<00:00, 46.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.3704\n",
            "\tValidation Loss: 1.8476\n",
            "Epoch [28/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 28: 100%|██████████| 1250/1250 [00:56<00:00, 22.02it/s]\n",
            "Validation Epoch 28: 100%|██████████| 32/32 [00:00<00:00, 46.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.2975\n",
            "\tValidation Loss: 2.2541\n",
            "Epoch [29/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 29: 100%|██████████| 1250/1250 [00:56<00:00, 22.20it/s]\n",
            "Validation Epoch 29: 100%|██████████| 32/32 [00:00<00:00, 46.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.2735\n",
            "\tValidation Loss: 1.3264\n",
            "Epoch [30/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 30: 100%|██████████| 1250/1250 [00:55<00:00, 22.40it/s]\n",
            "Validation Epoch 30: 100%|██████████| 32/32 [00:00<00:00, 48.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.2566\n",
            "\tValidation Loss: 1.6136\n",
            "Epoch [31/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 31: 100%|██████████| 1250/1250 [00:55<00:00, 22.68it/s]\n",
            "Validation Epoch 31: 100%|██████████| 32/32 [00:00<00:00, 48.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 3.1661\n",
            "\tValidation Loss: 1.6990\n",
            "Epoch [32/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 32: 100%|██████████| 1250/1250 [00:56<00:00, 22.24it/s]\n",
            "Validation Epoch 32: 100%|██████████| 32/32 [00:00<00:00, 49.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.8914\n",
            "\tValidation Loss: 1.1418\n",
            "New best model found at epoch 32 with validation loss: 1.1418\n",
            "Epoch [33/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 33: 100%|██████████| 1250/1250 [00:56<00:00, 22.25it/s]\n",
            "Validation Epoch 33: 100%|██████████| 32/32 [00:00<00:00, 49.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.8518\n",
            "\tValidation Loss: 1.1471\n",
            "Epoch [34/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 34: 100%|██████████| 1250/1250 [00:55<00:00, 22.36it/s]\n",
            "Validation Epoch 34: 100%|██████████| 32/32 [00:00<00:00, 48.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.8225\n",
            "\tValidation Loss: 0.8814\n",
            "New best model found at epoch 34 with validation loss: 0.8814\n",
            "Epoch [35/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 35: 100%|██████████| 1250/1250 [00:55<00:00, 22.39it/s]\n",
            "Validation Epoch 35: 100%|██████████| 32/32 [00:00<00:00, 49.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.7856\n",
            "\tValidation Loss: 1.0900\n",
            "Epoch [36/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 36: 100%|██████████| 1250/1250 [00:57<00:00, 21.92it/s]\n",
            "Validation Epoch 36: 100%|██████████| 32/32 [00:00<00:00, 47.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.7603\n",
            "\tValidation Loss: 0.8978\n",
            "Epoch [37/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 37: 100%|██████████| 1250/1250 [00:56<00:00, 21.94it/s]\n",
            "Validation Epoch 37: 100%|██████████| 32/32 [00:00<00:00, 45.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.7584\n",
            "\tValidation Loss: 1.0288\n",
            "Epoch [38/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 38: 100%|██████████| 1250/1250 [00:57<00:00, 21.62it/s]\n",
            "Validation Epoch 38: 100%|██████████| 32/32 [00:00<00:00, 44.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.7570\n",
            "\tValidation Loss: 1.2676\n",
            "Epoch [39/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 39: 100%|██████████| 1250/1250 [00:56<00:00, 22.00it/s]\n",
            "Validation Epoch 39: 100%|██████████| 32/32 [00:00<00:00, 46.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.7522\n",
            "\tValidation Loss: 1.1792\n",
            "Epoch [40/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 40: 100%|██████████| 1250/1250 [00:55<00:00, 22.40it/s]\n",
            "Validation Epoch 40: 100%|██████████| 32/32 [00:00<00:00, 50.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5921\n",
            "\tValidation Loss: 0.9328\n",
            "Epoch [41/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 41: 100%|██████████| 1250/1250 [00:55<00:00, 22.50it/s]\n",
            "Validation Epoch 41: 100%|██████████| 32/32 [00:00<00:00, 49.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5355\n",
            "\tValidation Loss: 0.8069\n",
            "New best model found at epoch 41 with validation loss: 0.8069\n",
            "Epoch [42/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 42: 100%|██████████| 1250/1250 [00:56<00:00, 22.11it/s]\n",
            "Validation Epoch 42: 100%|██████████| 32/32 [00:00<00:00, 48.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5702\n",
            "\tValidation Loss: 0.9021\n",
            "Epoch [43/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 43: 100%|██████████| 1250/1250 [00:55<00:00, 22.39it/s]\n",
            "Validation Epoch 43: 100%|██████████| 32/32 [00:00<00:00, 48.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5241\n",
            "\tValidation Loss: 0.9639\n",
            "Epoch [44/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 44: 100%|██████████| 1250/1250 [00:55<00:00, 22.41it/s]\n",
            "Validation Epoch 44: 100%|██████████| 32/32 [00:00<00:00, 47.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5525\n",
            "\tValidation Loss: 0.8848\n",
            "Epoch [45/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 45: 100%|██████████| 1250/1250 [00:55<00:00, 22.36it/s]\n",
            "Validation Epoch 45: 100%|██████████| 32/32 [00:00<00:00, 48.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5133\n",
            "\tValidation Loss: 0.9142\n",
            "Epoch [46/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 46: 100%|██████████| 1250/1250 [00:55<00:00, 22.46it/s]\n",
            "Validation Epoch 46: 100%|██████████| 32/32 [00:00<00:00, 45.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.5259\n",
            "\tValidation Loss: 0.8874\n",
            "Epoch [47/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 47: 100%|██████████| 1250/1250 [00:56<00:00, 21.98it/s]\n",
            "Validation Epoch 47: 100%|██████████| 32/32 [00:00<00:00, 50.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.4741\n",
            "\tValidation Loss: 0.7187\n",
            "New best model found at epoch 47 with validation loss: 0.7187\n",
            "Epoch [48/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 48: 100%|██████████| 1250/1250 [00:56<00:00, 22.09it/s]\n",
            "Validation Epoch 48: 100%|██████████| 32/32 [00:00<00:00, 46.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.4533\n",
            "\tValidation Loss: 0.7356\n",
            "Epoch [49/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 49: 100%|██████████| 1250/1250 [00:56<00:00, 22.22it/s]\n",
            "Validation Epoch 49: 100%|██████████| 32/32 [00:00<00:00, 45.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.4329\n",
            "\tValidation Loss: 0.8785\n",
            "Epoch [50/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 50: 100%|██████████| 1250/1250 [00:56<00:00, 22.32it/s]\n",
            "Validation Epoch 50: 100%|██████████| 32/32 [00:00<00:00, 44.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Loss: 2.4682\n",
            "\tValidation Loss: 0.9097\n",
            "Loaded checkpoint from: /content/drive/MyDrive/two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet_regression/torch_mobilenetv2/tmp/best_model.pth\n",
            "Evaluation Loss: 0.7187\n",
            "R-squared for target 0: 0.9567\n",
            "R-squared for target 1: 0.8745\n",
            "R-squared for target 2: 0.9378\n",
            "R-squared for target 3: 0.9070\n",
            "Training Run complete! Val loss = 0.7187 | Val R-squared (avg) = 0.9190 | Epoch = 47\n",
            "------------------------------\n",
            "New best model found at Run 1 with validation loss: 0.7187\n",
            "\n",
            "Loaded checkpoint from: /content/drive/MyDrive/two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet_regression/torch_mobilenetv2/final/best_model.pth\n",
            "Evaluation Loss: 0.7187\n",
            "R-squared for target 0: 0.9567\n",
            "R-squared for target 1: 0.8745\n",
            "R-squared for target 2: 0.9378\n",
            "R-squared for target 3: 0.9070\n",
            "Training complete! Final Val loss = 0.7187 | Final Val R-squared (avg) = 0.9190\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the best model is loaded and on the correct device\n",
        "# This part should be executed after the training section if you haven't already.\n",
        "model = load_mobilenetv2_regression(num_targets=num_regression_targets,\n",
        "                          pretrained=False,\n",
        "                        checkpoint_path=checkpoint_path / 'final' / 'best_model.pth')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval(); # Set the model to evaluation mode"
      ],
      "metadata": {
        "id": "87K88Xxh24ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b0ed21-4a60-490a-a4c1-648cd5d505a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from: /content/drive/MyDrive/two4two_sickones_models_pytorch/sick_ones_bendbias_v3_2class_normal/mobilenet_regression/torch_mobilenetv2/final/best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds, _, _ = evaluate_model(model, train_eval_dataloader, criterion, device, num_regression_targets)\n",
        "test_preds, _, _ = evaluate_model(model, test_dataloader, criterion, device, num_regression_targets)\n",
        "eval_preds, _, _ = evaluate_model(model, eval_dataloader, criterion, device, num_regression_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emQxpRVRJjzw",
        "outputId": "40c77faf-62bd-40bd-c933-2b6365376b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 2.3460\n",
            "R-squared for target 0: 0.8828\n",
            "R-squared for target 1: 0.6280\n",
            "R-squared for target 2: 0.7179\n",
            "R-squared for target 3: 0.6173\n",
            "Evaluation Loss: 0.7617\n",
            "R-squared for target 0: 0.9575\n",
            "R-squared for target 1: 0.8780\n",
            "R-squared for target 2: 0.9324\n",
            "R-squared for target 3: 0.9125\n",
            "Evaluation Loss: 2.9083\n",
            "R-squared for target 0: 0.8639\n",
            "R-squared for target 1: 0.5723\n",
            "R-squared for target 2: 0.4901\n",
            "R-squared for target 3: 0.4477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 5\n",
        "print(test_dataset[sample_idx][1])\n",
        "print(test_preds[sample_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVJIdNnjKh7Z",
        "outputId": "69768cca-1069-4fe4-d9cc-1f7960af8b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1597, 0.5904, 0.3541, 0.3838])\n",
            "[1.1382922  0.56760174 0.30983067 0.36736593]\n"
          ]
        }
      ]
    }
  ]
}